# NovaHub - AI Model Bundling Plan

**Goal:** Bundle a small, high-quality AI model (2-6GB) with NovaHub for instant out-of-box usage

**Status:** Planning Phase  
**Target Version:** 0.2.0

---

## Why Bundle a Model?

âœ… **Zero Setup** - Works immediately after install  
âœ… **No Internet Required** - Fully offline after install  
âœ… **Privacy First** - Code never leaves your machine  
âœ… **No API Keys** - Completely free to use  
âœ… **Fast** - No network latency  

---

## Model Selection Criteria

### Requirements:
- **Size:** 2-6GB maximum (fits on most systems)
- **Quality:** Good code generation and understanding
- **Speed:** Fast inference on CPU (< 3 seconds per response)
- **License:** Open source, commercial use allowed
- **Format:** GGUF or compatible with Ollama

### Hardware Targets:
- **Minimum:** 8GB RAM, any CPU
- **Recommended:** 16GB RAM, modern CPU
- **Optimal:** 16GB+ RAM, GPU support

---

## Model Candidates (2-6GB Range)

### ðŸ¥‡ Top Candidates

#### 1. Qwen2.5-Coder 3B (2GB)
- **Size:** ~2GB GGUF
- **Strengths:** Excellent code quality, 32K context, fast
- **Performance:** Great for coding tasks
- **License:** Apache 2.0
- **Status:** â­ **RECOMMENDED**

#### 2. Phi-3 Mini (2.3GB)
- **Size:** 2.3GB
- **Strengths:** Microsoft, very fast, good quality
- **Performance:** Good for general coding
- **License:** MIT
- **Status:** Strong candidate

#### 3. Deepseek-Coder 1.3B (800MB)
- **Size:** 800MB
- **Strengths:** Ultra lightweight, surprisingly good
- **Performance:** Fast, decent quality
- **License:** Apache 2.0
- **Status:** Backup option (very small)

#### 4. StarCoder2 3B (1.7GB)
- **Size:** 1.7GB
- **Strengths:** Hugging Face, great for code completion
- **Performance:** Fast, specialized for code
- **License:** Apache 2.0
- **Status:** Good alternative

#### 5. TinyLlama 1.1B (637MB)
- **Size:** 637MB
- **Strengths:** Extremely small, still functional
- **Performance:** Basic tasks, very fast
- **License:** Apache 2.0
- **Status:** Emergency backup

---

## Testing Plan

### Phase 1: Download & Test (Week 1)
```bash
# Test each candidate
ollama pull qwen2.5-coder:3b
ollama pull phi3:mini
ollama pull deepseek-coder:1.3b
ollama pull starcoder2:3b
ollama pull tinyllama
```

### Phase 2: Benchmark (Week 1-2)

**Test Cases:**
- Code generation (write a function)
- Code explanation (explain existing code)
- Bug fixing (find and fix bugs)
- Refactoring (improve code structure)
- Documentation (write docs)

**Metrics:**
- Response quality (1-10)
- Response speed (seconds)
- Context understanding
- Code correctness
- RAM usage
- CPU usage

### Phase 3: User Testing (Week 2)
- Real-world coding tasks
- Multiple programming languages
- Different complexity levels
- Edge cases

---

## Integration Plan

### Step 1: Model Selection
- [ ] Test all 5 candidates
- [ ] Benchmark performance
- [ ] Select winner (likely Qwen2.5-Coder 3B)

### Step 2: Packaging
- [ ] Download GGUF model file
- [ ] Optimize quantization (Q4_K_M recommended)
- [ ] Package with NovaHub installer
- [ ] Test extraction and loading

### Step 3: Installer Update
```bash
# Updated install.sh
- Download NovaHub
- Install Ollama
- Extract bundled model to ~/.ollama/models/
- Verify model works
- Set as default in config
```

### Step 4: UI Integration
- [ ] Show bundled model in provider list with "ðŸ“¦ Bundled"
- [ ] Display model size (e.g., "2GB - Bundled with NovaHub")
- [ ] Show hardware requirements
- [ ] Add model info dialog

### Step 5: Model Management
- [ ] Detect if bundled model exists
- [ ] Allow switching to downloaded models
- [ ] Show disk space usage
- [ ] Add "Download more models" button

---

## Custom Fine-Tuning (Future)

### Phase 1: Data Collection
- [ ] Gather NovaHub-specific coding patterns
- [ ] Collect user interactions (opt-in)
- [ ] Create training dataset
- [ ] Validate data quality

### Phase 2: Fine-Tuning
- [ ] Select base model (Qwen2.5-Coder 3B)
- [ ] Set up training pipeline (LoRA)
- [ ] Train for NovaHub workflows
- [ ] Test and validate improvements

### Phase 3: NovaHub Model v1
- [ ] Name: "NovaHub Coder 3B"
- [ ] Optimized for: CLI workflows, terminal context
- [ ] Special features: NovaHub command understanding
- [ ] Release as official NovaHub model

---

## Provider List with Model Sizes

### Proposed UI:

```
â”Œâ”€ Select Provider â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                           â”‚
â”‚  Local AI (No API Key)                                   â”‚
â”‚  â—‰ Ollama - Local AI                                     â”‚
â”‚    â””â”€ Browse Models...                                   â”‚
â”‚                                                           â”‚
â”‚  Bundled Models                                          â”‚
â”‚  ðŸ“¦ NovaHub Coder 3B (2GB) - Bundled â­                   â”‚
â”‚     Recommended â€¢ Fast â€¢ No download needed              â”‚
â”‚                                                           â”‚
â”‚  Downloaded Models                                       â”‚
â”‚  âœ“ qwen2.5-coder:7b (4.7GB)                              â”‚
â”‚     32K context â€¢ Great for coding                       â”‚
â”‚  âœ“ deepseek-coder-v2:16b (9GB)                           â”‚
â”‚     16K context â€¢ Most powerful                          â”‚
â”‚                                                           â”‚
â”‚  Available to Download                                   â”‚
â”‚  â¬‡ codellama:13b (7.4GB)                                 â”‚
â”‚     Meta's coding model                                  â”‚
â”‚  â¬‡ phi3:mini (2.3GB)                                     â”‚
â”‚     Microsoft â€¢ Lightweight                              â”‚
â”‚                                                           â”‚
â”‚  Cloud Providers                                         â”‚
â”‚  â—‹ Anthropic - Claude models (API key required)         â”‚
â”‚  â—‹ OpenAI - ChatGPT models (API key required)           â”‚
â”‚  â—‹ Google - Gemini models (API key required)            â”‚
â”‚                                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Model Info Display:
```
â”Œâ”€ Model Details: qwen2.5-coder:7b â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                           â”‚
â”‚  Size: 4.7GB                                             â”‚
â”‚  RAM Required: 8GB minimum, 16GB recommended             â”‚
â”‚  Context Window: 32,768 tokens                           â”‚
â”‚  Speed: ~2s per response (CPU)                           â”‚
â”‚  Cost: Free (local)                                      â”‚
â”‚                                                           â”‚
â”‚  Capabilities:                                           â”‚
â”‚  âœ“ Code generation                                       â”‚
â”‚  âœ“ Code explanation                                      â”‚
â”‚  âœ“ Bug fixing                                            â”‚
â”‚  âœ“ Refactoring                                           â”‚
â”‚  âœ“ Multiple languages                                    â”‚
â”‚                                                           â”‚
â”‚  [Download] [Set as Default] [Remove]                    â”‚
â”‚                                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Implementation Phases

### ðŸ”µ Phase 1: Ollama Integration (In Progress)
- [x] Install Ollama in installer
- [x] Configure Ollama provider
- [x] Add to provider list
- [ ] Test with models

### ðŸŸ¡ Phase 2: Model Selection (Week 1-2)
- [ ] Download all candidates
- [ ] Run benchmarks
- [ ] Select best model
- [ ] Document decision

### ðŸŸ  Phase 3: Bundling (Week 2-3)
- [ ] Package model with installer
- [ ] Auto-extract on install
- [ ] Verify it works
- [ ] Test on clean system

### ðŸ”´ Phase 4: UI Enhancement (Week 3-4)
- [ ] Show model sizes
- [ ] Display requirements
- [ ] Add download progress
- [ ] Model management UI

### ðŸŸ£ Phase 5: Fine-Tuning (Month 2)
- [ ] Collect training data
- [ ] Fine-tune base model
- [ ] Test NovaHub model
- [ ] Release v1

---

## Disk Space Management

### Default Installation:
```
NovaHub: ~200MB
Ollama: ~50MB
Bundled Model (Qwen 3B): ~2GB
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total: ~2.3GB
```

### With Additional Models:
```
+ qwen2.5-coder:7b: 4.7GB
+ deepseek-v2:16b: 9GB
+ codellama:13b: 7.4GB
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total: ~23.4GB (if all downloaded)
```

### Recommendations:
- **8GB RAM:** Bundled 3B model only (2.3GB total)
- **16GB RAM:** Add 7B model (7GB total)
- **32GB+ RAM:** Download multiple models (up to 25GB)

---

## Success Metrics

### For v0.2.0:
- [ ] Model bundled with installer
- [ ] Works offline immediately after install
- [ ] < 5 second response time on average CPU
- [ ] Accurate code generation (75%+ correctness)
- [ ] Runs on 8GB RAM systems

### For v1.0.0:
- [ ] Custom fine-tuned NovaHub model
- [ ] Advanced model management UI
- [ ] Multiple bundled models (small/medium/large)
- [ ] GPU acceleration support
- [ ] Model switching without restart

---

## Timeline

**Week 1:** Model selection and benchmarking  
**Week 2:** Integration and packaging  
**Week 3:** UI enhancements and testing  
**Week 4:** Release v0.2.0 with bundled model  
**Month 2:** Custom fine-tuning  
**Month 3:** Advanced features and v1.0.0

---

## Notes

- Start with Qwen2.5-Coder 3B (best size/quality ratio)
- Can always add more models later
- Users can still use cloud providers if they want
- Focus on ease of use and zero setup

---

**Status:** ðŸ“‹ Planning  
**Next Step:** Download and test model candidates  
**Target:** v0.2.0 release with bundled AI model
